#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨å­¦ä¹ ç‚¹å‡»å™¨
è‡ªåŠ¨è¯†åˆ«è¯¾ç¨‹å­¦æ—¶è¿›åº¦ï¼Œç‚¹å‡»æœªå®Œæˆçš„è¯¾ç¨‹
"""

import cv2
import numpy as np
import pyautogui
import time
import re
from PIL import Image

try:
    import pytesseract
    HAS_OCR = True
except ImportError:
    HAS_OCR = False
    print("âš ï¸  pytesseractæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ–¹æ³•")


class LearningAutoClicker:
    def __init__(self, image_path='lists.png'):
        self.image_path = image_path
        self.image = None
        self.level1_buttons = []  # ä¸€çº§èœå•æŒ‰é’®ï¼ˆçº¢è‰²ï¼‰
        self.level2_items = []     # äºŒçº§èœå•é¡¹
        
    def load_image(self):
        """åŠ è½½å›¾ç‰‡"""
        self.image = cv2.imread(self.image_path)
        if self.image is None:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾ç‰‡: {self.image_path}")
        print(f"âœ… å›¾ç‰‡å·²åŠ è½½: {self.image.shape[1]}x{self.image.shape[0]}")
    
    def detect_level1_menus(self):
        """æ£€æµ‹ä¸€çº§èœå•ï¼ˆçº¢è‰²æŒ‰é’®ï¼‰"""
        print("\nğŸ” æ£€æµ‹ä¸€çº§èœå•ï¼ˆçº¢è‰²æŒ‰é’®ï¼‰...")
        
        hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)
        
        # çº¢è‰²èŒƒå›´
        lower_red1 = np.array([0, 100, 100])
        upper_red1 = np.array([10, 255, 255])
        lower_red2 = np.array([160, 100, 100])
        upper_red2 = np.array([180, 255, 255])
        
        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)
        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)
        red_mask = cv2.bitwise_or(mask1, mask2)
        
        contours, _ = cv2.findContours(red_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        buttons = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if area > 500:  # æœ€å°é¢ç§¯
                x, y, w, h = cv2.boundingRect(contour)
                if w > 40 and h > 30:  # æœ€å°å°ºå¯¸
                    buttons.append({
                        'x': x,
                        'y': y,
                        'width': w,
                        'height': h,
                        'center_x': x + w // 2,
                        'center_y': y + h // 2,
                        'level': 1
                    })
        
        # æŒ‰Yåæ ‡æ’åº
        buttons.sort(key=lambda b: b['y'])
        
        # å»é‡ï¼ˆç›¸è¿‘çš„æŒ‰é’®åˆå¹¶ï¼‰
        unique_buttons = []
        for btn in buttons:
            is_duplicate = False
            for existing in unique_buttons:
                if abs(btn['center_y'] - existing['center_y']) < 50:
                    is_duplicate = True
                    break
            if not is_duplicate:
                unique_buttons.append(btn)
        
        self.level1_buttons = unique_buttons
        print(f"   æ‰¾åˆ° {len(self.level1_buttons)} ä¸ªä¸€çº§èœå•æŒ‰é’®")
        
        for i, btn in enumerate(self.level1_buttons):
            print(f"   èœå• {i+1}: ä½ç½® ({btn['center_x']}, {btn['center_y']})")
        
        return self.level1_buttons
    
    def extract_text_regions(self):
        """æå–æ‰€æœ‰æ–‡æœ¬åŒºåŸŸ"""
        print("\nğŸ“ æå–æ–‡æœ¬åŒºåŸŸ...")
        
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        
        # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY, 11, 2)
        
        # æŸ¥æ‰¾è½®å»“
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        text_regions = []
        for contour in contours:
            x, y, w, h = cv2.boundingRect(contour)
            # æ–‡æœ¬åŒºåŸŸç‰¹å¾ï¼šå®½åº¦è¾ƒå¤§ï¼Œé«˜åº¦é€‚ä¸­
            if 100 < w < 600 and 15 < h < 50:
                text_regions.append({
                    'x': x,
                    'y': y,
                    'width': w,
                    'height': h,
                    'center_x': x + w // 2,
                    'center_y': y + h // 2
                })
        
        print(f"   æ‰¾åˆ° {len(text_regions)} ä¸ªæ–‡æœ¬åŒºåŸŸå€™é€‰")
        return text_regions
    
    def ocr_learning_hours(self, region):
        """OCRè¯†åˆ«å­¦æ—¶ä¿¡æ¯"""
        if not HAS_OCR:
            return None
        
        try:
            # æå–åŒºåŸŸ
            x, y, w, h = region['x'], region['y'], region['width'], region['height']
            roi = self.image[y:y+h, x:x+w]
            
            # é¢„å¤„ç†
            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
            # å¢å¼ºå¯¹æ¯”åº¦
            gray = cv2.equalizeHist(gray)
            
            # OCR
            text = pytesseract.image_to_string(gray, lang='chi_sim+eng', 
                                              config='--psm 7')
            text = text.strip()
            
            # åŒ¹é…å­¦æ—¶æ ¼å¼ï¼šx.x/x.xå­¦æ—¶
            pattern = r'(\d+\.?\d*)/(\d+\.?\d*)å­¦æ—¶'
            match = re.search(pattern, text)
            
            if match:
                completed = float(match.group(1))
                total = float(match.group(2))
                return {
                    'text': text,
                    'completed': completed,
                    'total': total,
                    'is_incomplete': completed < total,
                    'region': region
                }
        except Exception as e:
            pass
        
        return None
    
    def detect_learning_hours_pattern(self):
        """æ£€æµ‹å­¦æ—¶æ¨¡å¼ï¼ˆä¸ä½¿ç”¨OCRçš„å¤‡ç”¨æ–¹æ³•ï¼‰"""
        print("\nğŸ” æ£€æµ‹å­¦æ—¶æ–‡æœ¬ï¼ˆæ¨¡å¼åŒ¹é…ï¼‰...")
        
        # åŸºäºä½ç½®å’Œé¢œè‰²ç‰¹å¾æ£€æµ‹
        gray = cv2.cvtColor(self.image, cv2.COLOR_BGR2GRAY)
        
        # æŸ¥æ‰¾åŒ…å«"/"å­—ç¬¦çš„åŒºåŸŸ
        # è¿™éœ€è¦æ¨¡æ¿åŒ¹é…æˆ–ç‰¹å®šçš„ç‰¹å¾æ£€æµ‹
        
        # ç®€åŒ–ç‰ˆï¼šåŸºäºä½ç½®æ¨æµ‹
        # äºŒçº§èœå•é€šå¸¸åœ¨ä¸€çº§èœå•çš„ä¸‹æ–¹ï¼Œå·¦ä¾§å¯¹é½
        learning_items = []
        
        # å¯¹æ¯ä¸ªä¸€çº§èœå•ï¼ŒæŸ¥æ‰¾å…¶ä¸‹æ–¹çš„å¯èƒ½äºŒçº§èœå•
        for i, btn in enumerate(self.level1_buttons):
            # äºŒçº§èœå•åŒºåŸŸï¼šä¸€çº§æŒ‰é’®ä¸‹æ–¹åˆ°ä¸‹ä¸€ä¸ªä¸€çº§æŒ‰é’®ä¹‹é—´
            y_start = btn['y'] + btn['height']
            if i + 1 < len(self.level1_buttons):
                y_end = self.level1_buttons[i + 1]['y']
            else:
                y_end = self.image.shape[0]
            
            # åœ¨è¿™ä¸ªåŒºåŸŸå†…æŸ¥æ‰¾æ–‡æœ¬
            region_height = y_end - y_start
            if region_height > 50:  # æœ‰è¶³å¤Ÿç©ºé—´
                # å‡è®¾äºŒçº§èœå•é¡¹çš„å‚ç›´é—´è·çº¦ä¸º40-50px
                num_items = max(1, (region_height - 50) // 45)
                
                for j in range(int(num_items)):
                    y_pos = y_start + 30 + j * 45
                    if y_pos < y_end - 20:
                        learning_items.append({
                            'level1_index': i,
                            'level2_index': j,
                            'x': 50,  # å·¦ä¾§ä½ç½®
                            'y': y_pos,
                            'center_x': 300,  # å¤§è‡´ä¸­å¿ƒ
                            'center_y': y_pos,
                            'parent_button': btn
                        })
        
        print(f"   æ£€æµ‹åˆ°çº¦ {len(learning_items)} ä¸ªå¯èƒ½çš„äºŒçº§èœå•é¡¹")
        return learning_items
    
    def analyze_and_visualize(self):
        """åˆ†æå¹¶å¯è§†åŒ–"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ è¯¾ç¨‹èœå•ç»“æ„åˆ†æ")
        print("=" * 70)
        
        self.load_image()
        
        # æ£€æµ‹ä¸€çº§èœå•
        level1_menus = self.detect_level1_menus()
        
        # å°è¯•OCRè¯†åˆ«å­¦æ—¶
        if HAS_OCR:
            text_regions = self.extract_text_regions()
            learning_hours = []
            
            print("\nğŸ“– OCRè¯†åˆ«å­¦æ—¶ä¿¡æ¯...")
            for region in text_regions:
                result = self.ocr_learning_hours(region)
                if result:
                    learning_hours.append(result)
            
            if learning_hours:
                print(f"\nâœ… è¯†åˆ«åˆ° {len(learning_hours)} ä¸ªå­¦æ—¶ä¿¡æ¯:")
                for i, item in enumerate(learning_hours):
                    status = "âŒ æœªå®Œæˆ" if item['is_incomplete'] else "âœ… å·²å®Œæˆ"
                    print(f"   {i+1}. {item['text']} - {status}")
                    print(f"      ä½ç½®: ({item['region']['center_x']}, {item['region']['center_y']})")
        else:
            # å¤‡ç”¨æ–¹æ³•
            learning_hours = self.detect_learning_hours_pattern()
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.create_visualization(level1_menus, learning_hours if HAS_OCR else [])
        
        return level1_menus, learning_hours if HAS_OCR else []
    
    def create_visualization(self, level1_menus, learning_hours):
        """åˆ›å»ºå¯è§†åŒ–å›¾"""
        result = self.image.copy()
        
        # ç»˜åˆ¶ä¸€çº§èœå•
        for i, menu in enumerate(level1_menus):
            cv2.rectangle(result, 
                         (menu['x'], menu['y']),
                         (menu['x'] + menu['width'], menu['y'] + menu['height']),
                         (0, 0, 255), 3)
            cv2.circle(result, (menu['center_x'], menu['center_y']), 8, (255, 0, 0), -1)
            cv2.putText(result, f"L1-{i+1}", 
                       (menu['x'] - 50, menu['center_y']),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # ç»˜åˆ¶å­¦æ—¶ä¿¡æ¯
        if HAS_OCR and learning_hours:
            for item in learning_hours:
                region = item['region']
                color = (0, 255, 0) if not item['is_incomplete'] else (0, 165, 255)  # ç»¿è‰²/æ©™è‰²
                
                cv2.rectangle(result,
                             (region['x'], region['y']),
                             (region['x'] + region['width'], region['y'] + region['height']),
                             color, 2)
                
                if item['is_incomplete']:
                    # æ ‡è®°éœ€è¦ç‚¹å‡»
                    cv2.circle(result, (region['center_x'], region['center_y']), 10, (0, 0, 255), -1)
                    cv2.putText(result, "CLICK!", 
                               (region['x'], region['y'] - 5),
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        
        # æ·»åŠ å›¾ä¾‹
        legend_y = 30
        cv2.putText(result, "Red Box: Level 1 Menu | Orange: Incomplete | Green: Complete",
                   (10, legend_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        output_path = 'learning_menu_analyzed.png'
        cv2.imwrite(output_path, result)
        print(f"\nâœ… å¯è§†åŒ–ç»“æœ: {output_path}")


def generate_click_script(level1_menus):
    """ç”Ÿæˆç‚¹å‡»è„šæœ¬"""
    script = """#!/usr/bin/env python3
# -*- coding: utf-8 -*-
\"\"\"
è‡ªåŠ¨å­¦ä¹ ç‚¹å‡»è„šæœ¬
æŒ‰ç…§èœå•å±‚çº§è‡ªåŠ¨ç‚¹å‡»æœªå®Œæˆçš„è¯¾ç¨‹
\"\"\"

import pyautogui
import time

# ä¸€çº§èœå•æŒ‰é’®åæ ‡ï¼ˆçº¢è‰²æŒ‰é’®ï¼‰
level1_buttons = [
"""
    
    for i, menu in enumerate(level1_menus):
        script += f"    {{'index': {i+1}, 'x': {menu['center_x']}, 'y': {menu['center_y']}}},  # èœå•{i+1}\n"
    
    script += """]

def click_level1_menu(menu_index, offset_x=0, offset_y=0):
    \"\"\"ç‚¹å‡»ä¸€çº§èœå•\"\"\"
    if 0 <= menu_index < len(level1_buttons):
        btn = level1_buttons[menu_index]
        x = offset_x + btn['x']
        y = offset_y + btn['y']
        
        print(f"ç‚¹å‡»ä¸€çº§èœå• {btn['index']}: ({x}, {y})")
        pyautogui.moveTo(x, y, duration=0.3)
        pyautogui.click()
        time.sleep(0.5)  # ç­‰å¾…èœå•å±•å¼€
    else:
        print(f"é”™è¯¯: èœå•ç´¢å¼•è¶…å‡ºèŒƒå›´")

def find_image_on_screen(image_path='lists.png'):
    \"\"\"åœ¨å±å¹•ä¸ŠæŸ¥æ‰¾å›¾ç‰‡ä½ç½®\"\"\"
    try:
        location = pyautogui.locateOnScreen(image_path, confidence=0.7)
        if location:
            return location.left, location.top
    except Exception as e:
        print(f"æŸ¥æ‰¾å›¾ç‰‡å¤±è´¥: {e}")
    return None, None

def auto_learn():
    \"\"\"è‡ªåŠ¨å­¦ä¹ æµç¨‹\"\"\"
    print("=" * 60)
    print("è‡ªåŠ¨å­¦ä¹ ç‚¹å‡»å™¨")
    print("=" * 60)
    
    # 1. æŸ¥æ‰¾å›¾ç‰‡ä½ç½®
    print("\\næ­¥éª¤1: åœ¨å±å¹•ä¸ŠæŸ¥æ‰¾è¯¾ç¨‹åˆ—è¡¨...")
    offset_x, offset_y = find_image_on_screen()
    
    if offset_x is None:
        print("âŒ æœªæ‰¾åˆ°è¯¾ç¨‹åˆ—è¡¨ï¼Œè¯·ç¡®ä¿æµè§ˆå™¨æ‰“å¼€å¹¶æ˜¾ç¤ºè¯¾ç¨‹é¡µé¢")
        return
    
    print(f"âœ… æ‰¾åˆ°ä½ç½®: ({offset_x}, {offset_y})")
    
    # 2. éå†ä¸€çº§èœå•
    print(f"\\næ­¥éª¤2: å¼€å§‹éå† {len(level1_buttons)} ä¸ªä¸€çº§èœå•...")
    
    for i in range(len(level1_buttons)):
        print(f"\\n>>> å¤„ç†èœå• {i+1}...")
        
        # ç‚¹å‡»ä¸€çº§èœå•å±•å¼€
        click_level1_menu(i, offset_x, offset_y)
        
        # è¿™é‡Œéœ€è¦æ·»åŠ äºŒçº§èœå•çš„è¯†åˆ«å’Œç‚¹å‡»é€»è¾‘
        # ç”±äºäºŒçº§èœå•åŠ¨æ€ç”Ÿæˆï¼Œéœ€è¦å®æ—¶æˆªå›¾åˆ†æ
        
        print(f"    ç­‰å¾…2ç§’è§‚å¯ŸäºŒçº§èœå•...")
        time.sleep(2)
        
        # TODO: è¯†åˆ«äºŒçº§èœå•ä¸­çš„ "x.x/x.xå­¦æ—¶" æ–‡æœ¬
        # TODO: æ¯”è¾ƒæ•°å­—ï¼Œå¦‚æœä¸ç›¸ç­‰åˆ™ç‚¹å‡»
        
    print("\\nâœ… å¤„ç†å®Œæˆï¼")

def manual_click_menu(menu_index):
    \"\"\"æ‰‹åŠ¨ç‚¹å‡»æŒ‡å®šèœå•\"\"\"
    print(f"\\nå‡†å¤‡ç‚¹å‡»èœå• {menu_index}...")
    print("è¯·ç¡®ä¿è¯¾ç¨‹åˆ—è¡¨åœ¨å±å¹•ä¸Šå¯è§")
    print("3ç§’åå¼€å§‹...")
    time.sleep(3)
    
    offset_x, offset_y = find_image_on_screen()
    if offset_x:
        click_level1_menu(menu_index - 1, offset_x, offset_y)
    else:
        print("æœªæ‰¾åˆ°è¯¾ç¨‹åˆ—è¡¨")

if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1:
        # æ‰‹åŠ¨ç‚¹å‡»æŒ‡å®šèœå•
        menu_num = int(sys.argv[1])
        manual_click_menu(menu_num)
    else:
        # è‡ªåŠ¨å­¦ä¹ æ¨¡å¼
        print("ç”¨æ³•:")
        print("  è‡ªåŠ¨æ¨¡å¼: python learning_click_script.py")
        print("  æ‰‹åŠ¨æ¨¡å¼: python learning_click_script.py <èœå•ç¼–å·>")
        print("\\nç¤ºä¾‹:")
        print("  python learning_click_script.py 1  # ç‚¹å‡»ç¬¬1ä¸ªèœå•")
        print()
        
        choice = input("å¯åŠ¨è‡ªåŠ¨å­¦ä¹ ? (y/n): ")
        if choice.lower() == 'y':
            auto_learn()
"""
    
    with open('learning_click_script.py', 'w', encoding='utf-8') as f:
        f.write(script)
    
    print("âœ… ç‚¹å‡»è„šæœ¬å·²ç”Ÿæˆ: learning_click_script.py")


def main():
    print("=" * 70)
    print("ğŸ“ åœ¨çº¿å­¦ä¹ è‡ªåŠ¨ç‚¹å‡»å™¨")
    print("=" * 70)
    print("\nåŠŸèƒ½è¯´æ˜:")
    print("  1. è¯†åˆ«ä¸€çº§èœå•ï¼ˆçº¢è‰²æŒ‰é’®ï¼‰")
    print("  2. è¯†åˆ«äºŒçº§èœå•ä¸­çš„å­¦æ—¶ä¿¡æ¯ (x.x/x.xå­¦æ—¶)")
    print("  3. è‡ªåŠ¨ç‚¹å‡»æœªå®Œæˆçš„è¯¾ç¨‹ (å·¦å³æ•°å­—ä¸ç›¸ç­‰)")
    print()
    
    if not HAS_OCR:
        print("âš ï¸  æç¤º: æœªå®‰è£…OCRåº“ï¼Œå»ºè®®å®‰è£…ä»¥è·å¾—æ›´å¥½çš„è¯†åˆ«æ•ˆæœ")
        print("   å®‰è£…å‘½ä»¤: pip install pytesseract")
        print("   è¿˜éœ€è¦å®‰è£…tesseract-ocrç³»ç»ŸåŒ…")
        print()
    
    clicker = LearningAutoClicker('lists.png')
    level1_menus, learning_hours = clicker.analyze_and_visualize()
    
    # ç”Ÿæˆè„šæœ¬
    generate_click_script(level1_menus)
    
    print("\n" + "=" * 70)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 70)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  1. learning_menu_analyzed.png    - å¯è§†åŒ–èœå•ç»“æ„")
    print("  2. learning_click_script.py      - è‡ªåŠ¨ç‚¹å‡»è„šæœ¬")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  æŸ¥çœ‹å¯è§†åŒ–: xdg-open learning_menu_analyzed.png")
    print("  è¿è¡Œè„šæœ¬: python learning_click_script.py")


if __name__ == '__main__':
    main()

